{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Mesh Parameterization: Exercises\n",
    "In this notebook you will perform the same analysis you did in 101 on more complex meshes, and try your hand at more complicated parameterization techniques. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Fixed Boundary Parameterization and Distortion Analysis ###\n",
    "In this folder you are given 2 meshes -- halfbunny.obj and ogre.obj. Load each of these meshes using gpytoolbox and use the code from notebook 101 to perform the analyses in the next few code blocks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading required packages\n",
    "import numpy as np\n",
    "import polyscope as ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 halfbunny.obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpytoolbox as gpy\n",
    "\n",
    "### TODO: use gpytoolbox to load in halfbunny.obj\n",
    "MESH_VERTICES, MESH_FACES = gpy.read_mesh(\"halfbunny.obj\")\n",
    "\n",
    "### TODO: Use the below code copied from notebook 101 to get the boundary and non-boundary edges of the imported mesh\n",
    "from igl import boundary_loop\n",
    "\n",
    "bnd = boundary_loop(MESH_FACES)\n",
    "boundary_idxs = list(sorted(bnd))\n",
    "\n",
    "# NOTE: pin the boundary to a circle -- can no longer define this by hand\n",
    "from igl import map_vertices_to_circle\n",
    "boundary_positions = map_vertices_to_circle(MESH_VERTICES, bnd).astype(MESH_VERTICES.dtype)\n",
    "\n",
    "# NOTE: Resort the positions to match the order of boundary_idxs\n",
    "boundary_positions = boundary_positions[np.argsort(bnd)]\n",
    "\n",
    "# vertices that aren't pinned\n",
    "pred_idxs = np.array([i for i in range(MESH_VERTICES.shape[0]) if i not in boundary_idxs])\n",
    "\n",
    "# # Get edge array (unique edges + counts)\n",
    "from collections import defaultdict\n",
    "edges = defaultdict(int)\n",
    "for f in MESH_FACES:\n",
    "    for i in range(3):\n",
    "        if f[i] > f[(i+1)%3]:\n",
    "            edges[(f[(i+1)%3], f[i])] += 1\n",
    "        else:\n",
    "            edges[(f[i], f[(i+1)%3])] += 1\n",
    "\n",
    "# # Valid edges are the ones that are shared by two faces\n",
    "tot_edges = np.array(list(edges.keys()))\n",
    "valid_edges = np.array([k for k, v in edges.items() if v == 2])\n",
    "boundary_edges = np.array([k for k, v in edges.items() if v == 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note in the example code that the process for computing the fixed boundary parameterizations for these meshes will be almost exactly the same as in exercise 101, except it is no longer so trivial to define the boundary positions. Instead, we will make use of libigl to fix the boundary vertices to a circle (standard convex domain). This is done using these additional lines in the example code\n",
    "\n",
    "```\n",
    "from igl import map_vertices_to_circle\n",
    "boundary_positions = map_vertices_to_circle(MESH_VERTICES, bnd).astype(MESH_VERTICES.dtype)\n",
    "boundary_positions = boundary_positions[np.argsort(bnd)]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Copy over the setup_parameterization_matrices() function from notebook 101 and use it to compute\n",
    "# 1) The Tutte parameterization (all weights of 1)\n",
    "# 2) The Mean Value weights parameterization (see formula and code for computing the mean value weights from notebook 101)\n",
    "\n",
    "def setup_parameterization_matrices(vertices, boundary_idxs, edges, weights=1):\n",
    "    \"\"\" Set up the Tutte linear system (laplacian weights of 1)\n",
    "\n",
    "    vertices (np.ndarray): V x 3 array of vertex positions\n",
    "    boundary_idxs (np.ndarray): B array of boundary vertex indices\n",
    "    edges (np.ndarray): E x 2 array of edge indices for weight assignment\n",
    "    weights (float or np.ndarray): E array of edge weights or scalar value (default 1)\n",
    "\n",
    "    Returns:\n",
    "        L (np.ndarray): V x V Laplacian matrix with boundary values set to 0\n",
    "        Lb (np.ndarray): V x B boundary Laplacian\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the laplacian matrix\n",
    "    L = np.zeros((vertices.shape[0], vertices.shape[0]))\n",
    "\n",
    "    L[edges[:, 0], edges[:, 1]] = weights\n",
    "    L[edges[:, 1], edges[:, 0]] = weights\n",
    "\n",
    "    # Off-diagonal elements are negative and diagonal is the negative sum of the row\n",
    "    L = np.diag(np.sum(L, axis=1)) - L\n",
    "\n",
    "    # Boundary weights are positive\n",
    "    Lb = -L[:, boundary_idxs]\n",
    "\n",
    "    # Boundary diagonal should be set to 0\n",
    "    Lb[boundary_idxs, range(len(boundary_idxs))] = 0\n",
    "\n",
    "    # Set the off-diagonal boundary columns to 0\n",
    "    Ldiag = np.diag(L).copy()\n",
    "\n",
    "    L[:, boundary_idxs] = 0\n",
    "    np.fill_diagonal(L, Ldiag)\n",
    "\n",
    "    return L, Lb\n",
    "\n",
    "L_tutte, Lb_tutte = setup_parameterization_matrices(MESH_VERTICES, boundary_idxs, valid_edges)\n",
    "uv_tutte = np.linalg.solve(L_tutte, Lb_tutte @ boundary_positions)\n",
    "uv_tutte[boundary_idxs] = boundary_positions\n",
    "\n",
    "meanvalues = {}\n",
    "for fi in range(len(MESH_FACES)):\n",
    "    f = MESH_FACES[fi]\n",
    "    for i in range(3):\n",
    "        v1 = f[i]\n",
    "        v2 = f[(i+1)%3]\n",
    "        v3 = f[(i+2)%3]\n",
    "        e1 = MESH_VERTICES[v2] - MESH_VERTICES[v1]\n",
    "        e2 = MESH_VERTICES[v3] - MESH_VERTICES[v1]\n",
    "\n",
    "        b1 = MESH_VERTICES[v1] - MESH_VERTICES[v2]\n",
    "        b2 = MESH_VERTICES[v3] - MESH_VERTICES[v2]\n",
    "\n",
    "        assert (v1, v2) not in meanvalues\n",
    "\n",
    "        alpha = np.arccos(np.dot(e1, e2) / (np.linalg.norm(e1) * np.linalg.norm(e2)))\n",
    "        beta = np.arccos(np.dot(b1, b2) / (np.linalg.norm(b1) * np.linalg.norm(b2)))\n",
    "\n",
    "        # Compute (tan(alpha_ij/2), tan(beta_ij/2), r_ij)\n",
    "        meanvalues[(v1, v2)] = (np.tan(alpha/2),\n",
    "                            np.tan(beta/2),\n",
    "                            np.linalg.norm(MESH_VERTICES[v2] - MESH_VERTICES[v1]))\n",
    "\n",
    "# For each edge pair, compute the mean value weights\n",
    "mean_value_weights = np.zeros((len(valid_edges),))\n",
    "for i, edge in enumerate(valid_edges):\n",
    "    v1, v2 = edge\n",
    "\n",
    "    assert (v1, v2) in meanvalues and (v2, v1) in meanvalues\n",
    "\n",
    "    a1, b2, r1 = meanvalues[(v1, v2)]\n",
    "    a2, b1, r2 = meanvalues[(v2, v1)]\n",
    "\n",
    "    assert r1 == r2\n",
    "\n",
    "    # tan(alpha_ij/2) + tan(beta_ji/2)\n",
    "    mean_value_weights[i] = (a1 + b1)/r1\n",
    "\n",
    "L_meanv, Lb_meanv = setup_parameterization_matrices(MESH_VERTICES, boundary_idxs, valid_edges, mean_value_weights)\n",
    "uv_meanv = np.linalg.solve(L_tutte, Lb_tutte @ boundary_positions)\n",
    "uv_meanv[boundary_idxs] = boundary_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polyscope as ps\n",
    "\n",
    "face_cols = np.arange(MESH_FACES.shape[0])\n",
    "\n",
    "# TODO: Visualize the mesh and the computed UV maps using polyscope\n",
    "ps.init()\n",
    "bunny = ps.register_surface_mesh(\"bunny\", MESH_VERTICES, MESH_FACES)\n",
    "tutte = ps.register_surface_mesh(\"bunny tutte\", uv_tutte + np.array([[3, 0]]), MESH_FACES)\n",
    "meanv = ps.register_surface_mesh(\"bunny meanv\", uv_meanv - np.array([[3, 0]]), MESH_FACES)\n",
    "\n",
    "bunny.add_scalar_quantity(\"color\", face_cols, \"faces\", enabled=True)\n",
    "tutte.add_scalar_quantity(\"color\", face_cols, \"faces\", enabled=True)\n",
    "meanv.add_scalar_quantity(\"color\", face_cols, \"faces\", enabled=True)\n",
    "\n",
    "ps.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1724\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# TODO: Copy over the get_jacobian() function from notebook 101 and use it to\n",
    "# compute the area, conformal, and isometric distortion for each parameterization.\n",
    "def get_jacobian(vs, fs, uvmap):\n",
    "\t\"\"\" Get jacobian of mesh given an input UV map\n",
    "\n",
    "\tArgs:\n",
    "\t\tvs (np.ndarray): V x 3 array of vertex positions\n",
    "\t\tfs (np.ndarray): F x 3 integer array of face indices\n",
    "\t\tuvmap (np.ndarray): V x 2 array of UV coordinates\n",
    "\n",
    "\tReturns:\n",
    "\t\t_type_: _description_\n",
    "\t\"\"\"\n",
    "\t# Visualize distortion\n",
    "\tfrom igl import grad\n",
    "\tG = np.array(grad(vs, fs).todense())\n",
    "\n",
    "\t# NOTE: currently gradient is organized as X1, X2, X3, ... Y1, Y2, Y3, ... Z1, Z2, Z3 ... reshape to X1, Y1, Z1, ...\n",
    "\tsplitind = G.shape[0]//3\n",
    "\tnewG = np.zeros_like(G) # F*3 x V\n",
    "\tnewG[::3] = G[:splitind]\n",
    "\tnewG[1::3] = G[splitind:2*splitind]\n",
    "\tnewG[2::3] = G[2*splitind:]\n",
    "\n",
    "\tJ = (newG @ uvmap).reshape(-1, 3, 2) # F x 3 x 2\n",
    "\treturn J\n",
    "\n",
    "def compute_energies(J):\n",
    "\t\"\"\" returns area, conformal, and isometric energies \"\"\"\n",
    "\n",
    "\t_, S, _ = np.linalg.svd(J)\n",
    "\n",
    "\tE_area = S[:, 0] * S[:, 1] + 1/(S[:, 0] * S[:, 1]) - 2\n",
    "\tE_conformal = (S[:, 0] - S[:, 1])**2\n",
    "\tE_isometric = S[:, 0]**2 + S[:, 1]**2 + 1/(S[:, 0]**2) + 1/(S[:, 1]**2) - 4\n",
    "\n",
    "\treturn E_area, E_conformal, E_isometric\n",
    "\n",
    "J_tutte = get_jacobian(MESH_VERTICES, MESH_FACES, uv_tutte)\n",
    "J_meanv = get_jacobian(MESH_VERTICES, MESH_FACES, uv_meanv)\n",
    "\n",
    "J_sparse = get_jacobian(MESH_VERTICES, MESH_FACES, uv_tutte)\n",
    "\n",
    "E_tutte = compute_energies(J_tutte)\n",
    "E_meanv = compute_energies(J_meanv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize the computed distortion energies using polyscope\n",
    "import polyscope as ps\n",
    "\n",
    "face_cols = np.arange(MESH_FACES.shape[0])\n",
    "\n",
    "# TODO: Visualize the mesh and the computed UV maps using polyscope\n",
    "ps.init()\n",
    "bunny = ps.register_surface_mesh(\"bunny\", MESH_VERTICES, MESH_FACES)\n",
    "tutte = ps.register_surface_mesh(\"bunny tutte\", uv_tutte + np.array([[3, 0]]), MESH_FACES)\n",
    "meanv = ps.register_surface_mesh(\"bunny meanv\", uv_meanv - np.array([[3, 0]]), MESH_FACES)\n",
    "\n",
    "bunny.add_scalar_quantity(\"color\", face_cols, \"faces\", enabled=False)\n",
    "tutte.add_scalar_quantity(\"color\", face_cols, \"faces\", enabled=False)\n",
    "meanv.add_scalar_quantity(\"color\", face_cols, \"faces\", enabled=False)\n",
    "\n",
    "tutte.add_scalar_quantity(\"area\", E_tutte[0], \"faces\")\n",
    "tutte.add_scalar_quantity(\"conformal\", E_tutte[1], \"faces\")\n",
    "tutte.add_scalar_quantity(\"isometric\", E_tutte[2], \"faces\")\n",
    "\n",
    "meanv.add_scalar_quantity(\"area\", E_meanv[0], \"faces\")\n",
    "meanv.add_scalar_quantity(\"conformal\", E_meanv[1], \"faces\")\n",
    "meanv.add_scalar_quantity(\"isometric\", E_meanv[2], \"faces\")\n",
    "\n",
    "ps.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 ogre.obj\n",
    "Note that this is a significantly larger mesh so expect `np.linalg.solve()` to take a few minutes to run! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpytoolbox as gpy\n",
    "\n",
    "### TODO: use gpytoolbox to load in halfbunny.obj\n",
    "MESH_VERTICES, MESH_FACES = gpy.read_mesh(\"ogre.obj\")\n",
    "\n",
    "### TODO: Use the below code copied from notebook 101 to get the boundary and non-boundary edges of the imported mesh\n",
    "from igl import boundary_loop\n",
    "\n",
    "bnd = boundary_loop(MESH_FACES)\n",
    "boundary_idxs = list(sorted(bnd))\n",
    "\n",
    "# NOTE: pin the boundary to a circle -- can no longer define this by hand\n",
    "from igl import map_vertices_to_circle\n",
    "boundary_positions = map_vertices_to_circle(MESH_VERTICES, bnd).astype(MESH_VERTICES.dtype)\n",
    "\n",
    "# NOTE: Resort the positions to match the order of boundary_idxs\n",
    "boundary_positions = boundary_positions[np.argsort(bnd)]\n",
    "\n",
    "# vertices that aren't pinned\n",
    "pred_idxs = np.array([i for i in range(MESH_VERTICES.shape[0]) if i not in boundary_idxs])\n",
    "\n",
    "# # Get edge array (unique edges + counts)\n",
    "from collections import defaultdict\n",
    "edges = defaultdict(int)\n",
    "for f in MESH_FACES:\n",
    "    for i in range(3):\n",
    "        if f[i] > f[(i+1)%3]:\n",
    "            edges[(f[(i+1)%3], f[i])] += 1\n",
    "        else:\n",
    "            edges[(f[i], f[(i+1)%3])] += 1\n",
    "\n",
    "# # Valid edges are the ones that are shared by two faces\n",
    "tot_edges = np.array(list(edges.keys()))\n",
    "valid_edges = np.array([k for k, v in edges.items() if v == 2])\n",
    "boundary_edges = np.array([k for k, v in edges.items() if v == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Copy over the setup_parameterization_matrices() function from notebook 101 and use it to compute\n",
    "# 1) The Tutte parameterization (all weights of 1)\n",
    "# 2) The Mean Value weights parameterization (see formula and code for computing the mean value weights from notebook 101)\n",
    "\n",
    "def setup_parameterization_matrices(vertices, boundary_idxs, edges, weights=1):\n",
    "    \"\"\" Set up the Tutte linear system (laplacian weights of 1)\n",
    "\n",
    "    vertices (np.ndarray): V x 3 array of vertex positions\n",
    "    boundary_idxs (np.ndarray): B array of boundary vertex indices\n",
    "    edges (np.ndarray): E x 2 array of edge indices for weight assignment\n",
    "    weights (float or np.ndarray): E array of edge weights or scalar value (default 1)\n",
    "\n",
    "    Returns:\n",
    "        L (np.ndarray): V x V Laplacian matrix with boundary values set to 0\n",
    "        Lb (np.ndarray): V x B boundary Laplacian\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the laplacian matrix\n",
    "    L = np.zeros((vertices.shape[0], vertices.shape[0]))\n",
    "\n",
    "    L[edges[:, 0], edges[:, 1]] = weights\n",
    "    L[edges[:, 1], edges[:, 0]] = weights\n",
    "\n",
    "    # Off-diagonal elements are negative and diagonal is the negative sum of the row\n",
    "    L = np.diag(np.sum(L, axis=1)) - L\n",
    "\n",
    "    # Boundary weights are positive\n",
    "    Lb = -L[:, boundary_idxs]\n",
    "\n",
    "    # Boundary diagonal should be set to 0\n",
    "    Lb[boundary_idxs, range(len(boundary_idxs))] = 0\n",
    "\n",
    "    # Set the off-diagonal boundary columns to 0\n",
    "    Ldiag = np.diag(L).copy()\n",
    "\n",
    "    L[:, boundary_idxs] = 0\n",
    "    np.fill_diagonal(L, Ldiag)\n",
    "\n",
    "    return L, Lb\n",
    "\n",
    "L_tutte, Lb_tutte = setup_parameterization_matrices(MESH_VERTICES, boundary_idxs, valid_edges)\n",
    "uv_tutte = np.linalg.solve(L_tutte, Lb_tutte @ boundary_positions)\n",
    "uv_tutte[boundary_idxs] = boundary_positions\n",
    "\n",
    "meanvalues = {}\n",
    "for fi in range(len(MESH_FACES)):\n",
    "    f = MESH_FACES[fi]\n",
    "    for i in range(3):\n",
    "        v1 = f[i]\n",
    "        v2 = f[(i+1)%3]\n",
    "        v3 = f[(i+2)%3]\n",
    "        e1 = MESH_VERTICES[v2] - MESH_VERTICES[v1]\n",
    "        e2 = MESH_VERTICES[v3] - MESH_VERTICES[v1]\n",
    "\n",
    "        b1 = MESH_VERTICES[v1] - MESH_VERTICES[v2]\n",
    "        b2 = MESH_VERTICES[v3] - MESH_VERTICES[v2]\n",
    "\n",
    "        assert (v1, v2) not in meanvalues\n",
    "\n",
    "        alpha = np.arccos(np.dot(e1, e2) / (np.linalg.norm(e1) * np.linalg.norm(e2)))\n",
    "        beta = np.arccos(np.dot(b1, b2) / (np.linalg.norm(b1) * np.linalg.norm(b2)))\n",
    "\n",
    "        # Compute (tan(alpha_ij/2), tan(beta_ij/2), r_ij)\n",
    "        meanvalues[(v1, v2)] = (np.tan(alpha/2),\n",
    "                            np.tan(beta/2),\n",
    "                            np.linalg.norm(MESH_VERTICES[v2] - MESH_VERTICES[v1]))\n",
    "\n",
    "# For each edge pair, compute the mean value weights\n",
    "mean_value_weights = np.zeros((len(valid_edges),))\n",
    "for i, edge in enumerate(valid_edges):\n",
    "    v1, v2 = edge\n",
    "\n",
    "    assert (v1, v2) in meanvalues and (v2, v1) in meanvalues\n",
    "\n",
    "    a1, b2, r1 = meanvalues[(v1, v2)]\n",
    "    a2, b1, r2 = meanvalues[(v2, v1)]\n",
    "\n",
    "    assert r1 == r2\n",
    "\n",
    "    # tan(alpha_ij/2) + tan(beta_ji/2)\n",
    "    mean_value_weights[i] = (a1 + b1)/r1\n",
    "\n",
    "L_meanv, Lb_meanv = setup_parameterization_matrices(MESH_VERTICES, boundary_idxs, valid_edges, mean_value_weights)\n",
    "uv_meanv = np.linalg.solve(L_tutte, Lb_tutte @ boundary_positions)\n",
    "uv_meanv[boundary_idxs] = boundary_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polyscope as ps\n",
    "\n",
    "face_cols = np.arange(MESH_FACES.shape[0])\n",
    "\n",
    "# TODO: Visualize the mesh and the computed UV maps using polyscope\n",
    "ps.init()\n",
    "bunny = ps.register_surface_mesh(\"bunny\", MESH_VERTICES, MESH_FACES)\n",
    "tutte = ps.register_surface_mesh(\"bunny tutte\", uv_tutte + np.array([[3, 0]]), MESH_FACES)\n",
    "meanv = ps.register_surface_mesh(\"bunny meanv\", uv_meanv - np.array([[3, 0]]), MESH_FACES)\n",
    "\n",
    "bunny.add_scalar_quantity(\"color\", face_cols, \"faces\", enabled=True)\n",
    "tutte.add_scalar_quantity(\"color\", face_cols, \"faces\", enabled=True)\n",
    "meanv.add_scalar_quantity(\"color\", face_cols, \"faces\", enabled=True)\n",
    "\n",
    "ps.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39856\n",
      "39856\n"
     ]
    }
   ],
   "source": [
    "# TODO: Copy over the get_jacobian() function from notebook 101 and use it to\n",
    "# compute the area, conformal, and isometric distortion for each parameterization.\n",
    "def get_jacobian_sp(vs, fs, uvmap):\n",
    "\t\"\"\" Get jacobian of mesh given an input UV map\n",
    "\n",
    "\tArgs:\n",
    "\t\tvs (np.ndarray): V x 3 array of vertex positions\n",
    "\t\tfs (np.ndarray): F x 3 integer array of face indices\n",
    "\t\tuvmap (np.ndarray): V x 2 array of UV coordinates\n",
    "\n",
    "\tReturns:\n",
    "\t\t_type_: _description_\n",
    "\t\"\"\"\n",
    "\t# Visualize distortion\n",
    "\tfrom igl import grad\n",
    "\n",
    "\tprint(fs.shape[0])\n",
    "\tG = grad(vs, fs)\n",
    "\n",
    "\tsplitind = G.shape[0]//3\n",
    "\n",
    "\t# NOTE: currently gradient is organized as X1, X2, X3, ... Y1, Y2, Y3, ... Z1, Z2, Z3 ... reshape to X1, Y1, Z1, ...\n",
    "\t# newG = np.zeros_like(G) # F*3 x V\n",
    "\t# newG[::3] = G[:splitind]\n",
    "\t# newG[1::3] = G[splitind:2*splitind]\n",
    "\t# newG[2::3] = G[2*splitind:]\n",
    "\n",
    "\tJ = (G * uvmap) # 3F x 2, (dX1, dX2, ...)\n",
    "\tnewJ = np.zeros((splitind, 3, 2))\n",
    "\tnewJ[:, 0] = J[:splitind]\n",
    "\tnewJ[:, 1] = J[splitind:2*splitind]\n",
    "\tnewJ[:, 2] = J[2*splitind:]\n",
    "\n",
    "\treturn newJ\n",
    "\n",
    "def compute_energies(J):\n",
    "\t\"\"\" returns area, conformal, and isometric energies \"\"\"\n",
    "\n",
    "\t_, S, _ = np.linalg.svd(J)\n",
    "\n",
    "\tE_area = S[:, 0] * S[:, 1] + 1/(S[:, 0] * S[:, 1]) - 2\n",
    "\tE_conformal = (S[:, 0] - S[:, 1])**2\n",
    "\tE_isometric = S[:, 0]**2 + S[:, 1]**2 + 1/(S[:, 0]**2) + 1/(S[:, 1]**2) - 4\n",
    "\n",
    "\treturn E_area, E_conformal, E_isometric\n",
    "\n",
    "J_tutte = get_jacobian_sp(MESH_VERTICES, MESH_FACES, uv_tutte)\n",
    "J_meanv = get_jacobian_sp(MESH_VERTICES, MESH_FACES, uv_meanv)\n",
    "\n",
    "E_tutte = compute_energies(J_tutte)\n",
    "E_meanv = compute_energies(J_meanv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize the computed distortion energies using polyscope\n",
    "import polyscope as ps\n",
    "\n",
    "face_cols = np.arange(MESH_FACES.shape[0])\n",
    "\n",
    "# TODO: Visualize the mesh and the computed UV maps using polyscope\n",
    "ps.init()\n",
    "bunny = ps.register_surface_mesh(\"bunny\", MESH_VERTICES, MESH_FACES)\n",
    "tutte = ps.register_surface_mesh(\"bunny tutte\", uv_tutte + np.array([[3, 0]]), MESH_FACES)\n",
    "meanv = ps.register_surface_mesh(\"bunny meanv\", uv_meanv - np.array([[3, 0]]), MESH_FACES)\n",
    "\n",
    "bunny.add_scalar_quantity(\"color\", face_cols, \"faces\", enabled=False)\n",
    "tutte.add_scalar_quantity(\"color\", face_cols, \"faces\", enabled=False)\n",
    "meanv.add_scalar_quantity(\"color\", face_cols, \"faces\", enabled=False)\n",
    "\n",
    "tutte.add_scalar_quantity(\"area\", E_tutte[0], \"faces\")\n",
    "tutte.add_scalar_quantity(\"conformal\", E_tutte[1], \"faces\")\n",
    "tutte.add_scalar_quantity(\"isometric\", E_tutte[2], \"faces\")\n",
    "\n",
    "meanv.add_scalar_quantity(\"area\", E_meanv[0], \"faces\")\n",
    "meanv.add_scalar_quantity(\"conformal\", E_meanv[1], \"faces\")\n",
    "meanv.add_scalar_quantity(\"isometric\", E_meanv[2], \"faces\")\n",
    "\n",
    "ps.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: LSCM and ARAP ###\n",
    "In this part you will use two more advanced parameterization techniques to flatten the same meshes. \n",
    "\n",
    "These two methods are Least Squares Conformal Maps [(LSCM)](https://www.cs.jhu.edu/~misha/Fall09/Levy02.pdf) and As-Rigid-As-Possible Mesh Parameterization [(ARAP)](https://cs.harvard.edu/~sjg/papers/arap.pdf). \n",
    "\n",
    "As you will see, these are two examples of **free boundary** methods (though LSCM technically requires two vertices to be pinned), which allows the boundary to move independently to perform the desired distortion minimization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Least Squares Conformal Maps [(LSCM)](https://www.cs.jhu.edu/~misha/Fall09/Levy02.pdf)\n",
    "As the name implies, LSCM is a **conformal** method, meaning it aims to minimize the conformal (angular) distortion of the parameterization, using a least squares solve. Deriving and computing this method by hand is beyond the scope of this exercise, so we will be using libigl's `lscm()` function to do the computation for us. \n",
    "\n",
    "One important note is that LSCM requires two vertices to be pinned in the plane to make the least-squared system well-determined (so there is a unique solution). Technically any two vertices can be chosen, but in practice vertices at the opposite end of a boundary loop are usually the best choice for method performance. The code commented below gives an example of computing the LSCM parameterization using libigl. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use the below example code to compute the LSCM parameterization of halfbunny.obj and ogre.obj\n",
    "# from igl import boundary_loop, lscm\n",
    "\n",
    "# bdry = boundary_loop(mesh.faces)\n",
    "\n",
    "# b = np.array([bdry[0], bdry[int(len(bdry)/2)]], dtype=\"int\")\n",
    "# bc = np.array([[0, 0], [1, 1]], dtype=np.float32)\n",
    "# succ, lscm_uv, error = lscm(mesh.vertices, mesh.faces, b, bc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute the area, conformal, and isometric distortion for the LSCM parameterization of halfbunny.obj and ogre.obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare the distortions of the LSCM results against the Tutte and Mean Value weights parameterizations\n",
    "# The LSCM conformal result should be close to 0. What about the area and isometric distortions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize the distortion values using Polyscope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 As-Rigid-As-Possible Mesh Parameterization [(ARAP)](https://cs.harvard.edu/~sjg/papers/arap.pdf)\n",
    "The ARAP method aims to minimize isometric distortion (both area and angles), using a non-linear algorithm which alternates between local and global optimization steps. The method requires an initial UV map as input, so we use a harmonic parameterization as an initial guess. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use the below example code to compute the ARAP parameterization of halfbunny.obj and ogre.obj\n",
    "# from igl import ARAP, boundary_loop, harmonic, map_vertices_to_circle\n",
    "# bnd = boundary_loop(mesh.faces)\n",
    "# bnd_uv = map_vertices_to_circle(mesh.vertices, bnd).astype(mesh.vertices.dtype)\n",
    "# initial_uv = harmonic(mesh.vertices, mesh.faces, bnd, bnd_uv, 1)\n",
    "# arap = ARAP(mesh.vertices, mesh.faces, 2, np.zeros(0), with_dynamics=True)\n",
    "# arap_uv = arap.solve(np.zeros((0,0)), initial_uv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute the area, conformal, and isometric distortion for the LSCM parameterization of halfbunny.obj and ogre.obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare the distortions of the ARAP results against the LSCM parameterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize the distortion values using Polyscope"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sgi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
